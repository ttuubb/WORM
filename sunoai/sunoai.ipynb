{
 "cells": [
  {
   "cell_type": "code",
   "id": "a9b5305c",
   "metadata": {},
   "source": [
    "#基础数据\n",
    "import requests\n",
    "import pandas as  pd\n",
    "import json\n",
    "proxy = {\n",
    "    \"http\": \"http://127.0.0.1:10808\",\n",
    "    \"https\": \"http://127.0.0.1:10808\",\n",
    "}\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "url = 'https://studio-api.prod.suno.com/api/discover'\n",
    "session = requests.Session()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e0cca16",
   "metadata": {},
   "source": [
    "#请求函数\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def make_suno_discover_request(body: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    url = \"https://studio-api.prod.suno.com/api/discover\"\n",
    "    headers = {\n",
    "        \"accept\": \"*/*\",\n",
    "        \"accept-language\": \"zh-CN,zh;q=0.9\",\n",
    "        \"affiliate-id\": \"undefined\",\n",
    "        \"authorization\": \"Bearer null\",\n",
    "        # \"browser-token\": \"{\\\"token\\\":\\\"eyJ0aW1lc3RhbXAiOjE3NDk1NjM4NTE3Nzl9\\\"}\",\n",
    "        \"cache-control\": \"no-cache\",\n",
    "        \"content-type\": \"text/plain;charset=UTF-8\",\n",
    "        # \"device-id\": \"94f64167-9cb1-4c77-99f0-33035be655f1\",\n",
    "        \"pragma\": \"no-cache\",\n",
    "        \"priority\": \"u=1, i\",\n",
    "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"137\\\", \\\"Chromium\\\";v=\\\"137\\\", \\\"Not/A)Brand\\\";v=\\\"24\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?1\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Android\\\"\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-site\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(body),proxies=proxy)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求发生错误: {e}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "92687881",
   "metadata": {},
   "source": [
    "#列表查重\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def update_suno_tenders_csv(new_data_list: List[Dict[str, Any]], csv_file_path: str) -> None:\n",
    "    expected_columns = ['title', 'video_url']\n",
    "\n",
    "    # 确保CSV文件存在，如果不存在则创建一个空的\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        if new_data_list:\n",
    "            # 明确指定列名\n",
    "            initial_df = pd.DataFrame(columns=expected_columns)\n",
    "            initial_df.to_csv(csv_file_path, index=False)\n",
    "        else:\n",
    "            print(\"新数据列表为空，无法创建带有列名的CSV文件。请手动创建或提供至少一条数据。\")\n",
    "            return\n",
    "\n",
    "    # 读取现有的CSV文件\n",
    "    # 读取现有的CSV文件\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_file_path, header=None, names=expected_columns)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        existing_df = pd.DataFrame(columns=expected_columns) # 如果文件为空，则创建一个空的DataFrame，并指定列名\n",
    "    except Exception as e:\n",
    "        print(f\"读取CSV文件时发生错误: {e}\")\n",
    "        existing_df = pd.DataFrame(columns=expected_columns)\n",
    "    # 将新数据转换为DataFrame\n",
    "    new_df = pd.DataFrame(new_data_list)\n",
    "\n",
    "    # 确保新数据DataFrame包含所有预期列，并按顺序排列\n",
    "    # 如果new_df中缺少video_url，则添加一个空字符串列\n",
    "    for col in expected_columns:\n",
    "        if col not in new_df.columns:\n",
    "            new_df[col] = ''\n",
    "    new_df = new_df[expected_columns] # 重新排序以匹配预期\n",
    "\n",
    "    # 检查新数据中是否有重复的title与现有数据重复\n",
    "    if not existing_df.empty:\n",
    "        existing_titles = set(existing_df['title'].astype(str))\n",
    "        # 过滤掉新数据中title已存在于现有数据中的条目\n",
    "        filtered_new_df = new_df[~new_df['title'].astype(str).isin(existing_titles)]\n",
    "    else:\n",
    "        filtered_new_df = new_df\n",
    "\n",
    "    # 将过滤后的新数据追加到CSV文件\n",
    "    if not filtered_new_df.empty:\n",
    "        filtered_new_df.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "        print(f\"成功将 {len(filtered_new_df)} 条新数据追加到 {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"没有新的不重复数据需要追加。\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34a1d0e8",
   "metadata": {},
   "source": [
    "#下载函数\n",
    "import yt_dlp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def hook(self, d):\n",
    "        if d['status'] == 'downloading':\n",
    "            if self.total is None:\n",
    "                self.total = d.get('total_bytes') or d.get('total_bytes_estimate', 0)\n",
    "            self.update(d['downloaded_bytes'] - self.n)\n",
    "        elif d['status'] == 'finished':\n",
    "            self.close()\n",
    "\n",
    "def download_videos(download_path: str, url_list: list):\n",
    "    \"\"\"\n",
    "    下载给定列表中的视频链接，支持断点续传，跳过已存在视频，实时显示下载进度，并使用代理。\n",
    "\n",
    "    Args:\n",
    "        download_path (str): 视频下载保存的目录。\n",
    "        url_list (list): 包含视频URL的列表。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "        print(f\"创建下载目录: {download_path}\")\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo+bestaudio/best',\n",
    "        'outtmpl': os.path.join(download_path, '%(title)s.%(ext)s'),\n",
    "        'noplaylist': True,  # 不下载播放列表\n",
    "        'continue': True,    # 启用断点续传\n",
    "        'download_archive': os.path.join(download_path, 'downloaded_videos.txt'), # 记录已下载的视频，用于跳过\n",
    "        'progress_hooks': [DownloadProgressBar(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=\"下载进度\").hook],\n",
    "        'proxy': 'http://127.0.0.1:10808', # 使用本地10808端口代理\n",
    "        'ignoreerrors': True, # 忽略下载错误，继续下载下一个\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        for url in url_list:\n",
    "            print(f\"\\n开始下载: {url}\")\n",
    "            try:\n",
    "                ydl.download([url])\n",
    "            except Exception as e:\n",
    "                print(f\"下载 {url} 时发生错误: {e}\")\n",
    "        print(\"\\n所有视频下载完成。\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0683284",
   "metadata": {},
   "source": [
    "# 每日热门\n",
    "body: Dict[str, Any] = {\n",
    "    \"start_index\": 1,\n",
    "    \"page_size\": 1,\n",
    "    \"section_name\": None,\n",
    "    \"section_content\": None,\n",
    "    \"secondary_section_content\": None,\n",
    "    \"product\": None\n",
    "}\n",
    "data = make_suno_discover_request(body)\n",
    "songs = data['sections'][0]['items'][2]['items']\n",
    "song_data = pd.DataFrame({\n",
    "    'title': [item['title'] for item in songs],\n",
    "    'video_url': [item['video_url'] for item in songs],\n",
    "    })\n",
    "\n",
    "print(song_data.to_dict(orient='records'))\n",
    "update_suno_tenders_csv(song_data.to_dict(orient='records'), 'suno_tenders.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c296a50e",
   "metadata": {},
   "source": [
    "# 每周热门\n",
    "body:Dict[str,Any]={\n",
    "  \"start_index\":0,\n",
    "  \"page_size\":1,\n",
    "  \"section_name\":\"trending_songs\",\n",
    "  \"section_content\":\"Global\",\n",
    "  \"secondary_section_content\":\"Weekly\",\n",
    "  \"page\":1,\n",
    "  \"section_size\":20,\n",
    "  \"disable_shuffle\":False\n",
    "}\n",
    "data = make_suno_discover_request(body)\n",
    "songs = data['sections'][0]['items']\n",
    "song_data = pd.DataFrame({\n",
    "    'title': [item['title'] for item in songs],\n",
    "    'video_url': [item['video_url'] for item in songs],\n",
    "    })\n",
    "\n",
    "print(song_data.to_dict(orient='records'))\n",
    "update_suno_tenders_csv(song_data.to_dict(orient='records'), 'suno_tenders.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "414a39c1",
   "metadata": {},
   "source": [
    "# 每月热门\n",
    "body:Dict[str,Any]={\n",
    "  \"start_index\":0,\n",
    "  \"page_size\":1,\n",
    "  \"section_name\":\"trending_songs\",\n",
    "  \"section_content\":\"Global\",\n",
    "  \"secondary_section_content\":\"Monthly\",\n",
    "  \"page\":1,\n",
    "  \"section_size\":20,\n",
    "  \"disable_shuffle\":False\n",
    "}\n",
    "data = make_suno_discover_request(body)\n",
    "songs = data['sections'][0]['items']\n",
    "song_data = pd.DataFrame({\n",
    "    'title': [item['title'] for item in songs],\n",
    "    'video_url': [item['video_url'] for item in songs],\n",
    "    })\n",
    "print(song_data.to_dict(orient='records'))\n",
    "update_suno_tenders_csv(song_data.to_dict(orient='records'), 'suno_tenders.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aea2aa39",
   "metadata": {},
   "source": [
    "# 总热门\n",
    "body:Dict[str,Any]={\n",
    "    \"start_index\":0,\"page_size\":1,\n",
    "    \"section_name\":\"trending_songs\",\n",
    "    \"section_content\":\"Global\",\n",
    "    \"secondary_section_content\":\"All Time\",\n",
    "    \"page\":1,\n",
    "    \"section_size\":20,\n",
    "    \"disable_shuffle\":False\n",
    "}\n",
    "data = make_suno_discover_request(body)\n",
    "songs = data['sections'][0]['items']\n",
    "song_data = pd.DataFrame({\n",
    "    'title': [item['title'] for item in songs],\n",
    "    'video_url': [item['video_url'] for item in songs],\n",
    "    })\n",
    "\n",
    "print(song_data.to_dict(orient='records'))\n",
    "update_suno_tenders_csv(song_data.to_dict(orient='records'), 'suno_tenders.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b0b8346",
   "metadata": {},
   "source": [
    "#v4.5\n",
    "body: Dict[str, Any] ={\n",
    "    \"start_index\": 2,\n",
    "    \"page_size\": 1,\n",
    "    \"section_name\": None,\n",
    "    \"section_content\": None,\n",
    "    \"secondary_section_content\": None,\n",
    "    \"product\": None\n",
    "}\n",
    "data = make_suno_discover_request(body)\n",
    "songs=data['sections'][0]['items']\n",
    "song_data = pd.DataFrame({\n",
    "    'title': [item['title'] for item in songs],\n",
    "    'video_url': [item['video_url'] for item in songs],\n",
    "})\n",
    "print(song_data.to_dict(orient='records'))\n",
    "update_suno_tenders_csv(song_data.to_dict(orient='records'), 'suno_tenders.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d59be6b",
   "metadata": {},
   "source": [
    "# 下载视频\n",
    "songs = pd.read_csv('suno_tenders.csv', header=None, names=['title', 'video_url']).to_dict(orient='records')\n",
    "song_urls = [song['video_url'] for song in songs]\n",
    "download_path = 'suno_videos'  # 视频下载保存的目录\n",
    "# 下载视频\n",
    "download_path = '/media/ttuubb/1C49-4CEA/sunoai'  # 视频下载保存的目录\n",
    "download_videos(download_path, song_urls)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
